{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8bef67e-f94d-4522-bc77-fa16ef8ca3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 19:41:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poojaakki/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "# conf.set('spark.ui.proxyBase'\n",
    "# , '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4041')\n",
    "conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "conf.set('spark.driver.memory','4g')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.SQLContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52b73545-e2e0-4db7-a571-5e8c303b3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "schema= StructType(\n",
    "      [StructField('title',StringType(),True),\n",
    "      StructField('text',StringType(),True),\n",
    "      StructField('subject',StringType(),True),\n",
    "      StructField('date',StringType(),True)])\n",
    "\n",
    "df0 = pd.read_csv('/Users/poojaakki/Desktop/projects/Fake-News-Detection-Data-Analysis/datasets/Kaggle/fake.csv')\n",
    "df1 = pd.read_csv('/Users/poojaakki/Desktop/projects/Fake-News-Detection-Data-Analysis/datasets/Kaggle/true.csv')\n",
    "df_fake = spark.createDataFrame(df0,schema=schema)\n",
    "df_true = spark.createDataFrame(df1,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54de89ac-6ac2-4559-a965-5c15485efbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fake.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "304887a4-614c-4e41-bdd0-c6e2c59b0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_true.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47d9d708-63ea-4d44-9651-8d59165e9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a label to the data for fake news as 0 and true news as 1 and shuffle using rand\n",
    "from pyspark.sql.functions import lit, rand\n",
    "\n",
    "df= df_true.withColumn('flag', lit(1)).union(df_fake.withColumn('flag', lit(0))).orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fef51cca-ad22-4b64-9d26-6fc0d05d9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c6c2eb9-6523-48e9-b0a5-228d4b3db576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 22:05:21 WARN TaskSetManager: Stage 59 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/04 22:05:21 WARN TaskSetManager: Stage 62 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>subject</th><th>count</th></tr>\n",
       "<tr><td>politicsNews</td><td>11272</td></tr>\n",
       "<tr><td>worldnews</td><td>10145</td></tr>\n",
       "<tr><td>News</td><td>9050</td></tr>\n",
       "<tr><td>politics</td><td>6841</td></tr>\n",
       "<tr><td>left-news</td><td>4459</td></tr>\n",
       "<tr><td>Government News</td><td>1570</td></tr>\n",
       "<tr><td>US_News</td><td>783</td></tr>\n",
       "<tr><td>Middle-east</td><td>778</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------------+-----+\n",
       "|        subject|count|\n",
       "+---------------+-----+\n",
       "|   politicsNews|11272|\n",
       "|      worldnews|10145|\n",
       "|           News| 9050|\n",
       "|       politics| 6841|\n",
       "|      left-news| 4459|\n",
       "|Government News| 1570|\n",
       "|        US_News|  783|\n",
       "|    Middle-east|  778|\n",
       "+---------------+-----+"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#types of news and highest number of news in order\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.groupby('subject').count().sort(col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e92245b8-0876-4eba-82d6-ff9fe16368df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text pre-processing\n",
    "\n",
    "from pyspark.sql.functions import col, split, lower, regexp_replace, length\n",
    "\n",
    "df = df.select('title',(lower(regexp_replace('text', \"[^a-zA-Z\\\\s]\", \"\")).alias('text')), 'subject', 'date', 'flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "afed8e9a-409b-4e9b-830e-e027d3d694c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words_token')\n",
    "\n",
    "df = tokenizer.transform(df).select('title','words_token','subject','date','flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14f450f5-b0f2-495a-8d06-f3e02cd2aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aaf20a61-5661-48de-bf60-a019d1ad4b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the stop words\n",
    "\n",
    "# tokens=['paris', 'reuters', '', 'plans', 'to', 'buy', 'a', 'new', 'presidential', 'jet', 'for', 'france', 's', 'emmanuel', 'macron', 'could', 'be']\n",
    "\n",
    "# clean_tokens = [ tok for tok in tokens if tok ]\n",
    "\n",
    "# clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5fcbfe70-d172-4436-b3d2-b460542502b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty string from the frame\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "def space_removal(f):\n",
    "    clean_tokens = []\n",
    "    for tok in f:\n",
    "        if tok:\n",
    "            clean_tokens.append(tok)\n",
    "    return clean_tokens\n",
    "\n",
    "udf_space_removal = udf(space_removal, ArrayType(StringType()))\n",
    "\n",
    "df = df.withColumn('words_token', udf_space_removal(f.col('words_token')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86ab0b29-4320-459f-8ea4-12f06f6df2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "92ad4e15-b56a-4eb7-85a1-e50299b52fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/poojaakki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f9056b40-9d7f-403c-8244-0f5e51d3dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the stop words\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stop_words_remover = StopWordsRemover(inputCol='words_token', outputCol='text').setStopWords(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b74dc0ff-7c4b-412c-8db9-c608dade0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stopWordRemovalPipeline = Pipeline(stages=[stop_words_remover])\n",
    "pipelineFitRemoveStopWords = stopWordRemovalPipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "04e09d68-33c7-47d1-a1c0-ed3cf1740ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipelineFitRemoveStopWords.transform(df.dropna()) #added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "80ea64a0-9283-4a15-bf05-7f0d44f09bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e777fe51-9f6c-4216-9015-234d02a9d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stematizing the words\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# stemmer = SnowballStemmer(language='english')\n",
    "# stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "\n",
    "# df = df.withColumn(\"words_stemmed\", stemmer_udf(\"text\")).select('title','words_token','subject','date','flag','words_stemmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc89562f-1be3-47bb-bfec-db9a28c4949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "94ddc611-08c4-4135-a67a-a1ef4b66010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.withColumnRenamed(\"words_stemmed\",\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "37ccd01f-dee0-4787-96e8-61c187a04a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 22:05:54 WARN TaskSetManager: Stage 65 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 22:05:57 WARN TaskSetManager: Stage 66 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 22:06:00 WARN TaskSetManager: Stage 69 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 22:06:03 WARN TaskSetManager: Stage 70 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>title</th><th>words_token</th><th>subject</th><th>date</th><th>flag</th><th>text</th></tr>\n",
       "<tr><td> Mueller Spokesma...</td><td>[trump, supporter...</td><td>News</td><td>December 17, 2017</td><td>0</td><td>[trump, supporter...</td></tr>\n",
       "<tr><td>JUST IN: FCC VOTE...</td><td>[today, the, fcc,...</td><td>politics</td><td>Dec 14, 2017</td><td>0</td><td>[today, fcc, vote...</td></tr>\n",
       "<tr><td>(VIDEO) OBAMA ON ...</td><td>[the, stats, don,...</td><td>politics</td><td>May 5, 2015</td><td>0</td><td>[stats, lie, one,...</td></tr>\n",
       "<tr><td>BREAKING VIDEO: T...</td><td>[the, attack, on,...</td><td>politics</td><td>Dec 19, 2016</td><td>0</td><td>[attack, russian,...</td></tr>\n",
       "<tr><td>3 CONSERVATIVE CE...</td><td>[ted, nugent, is,...</td><td>left-news</td><td>Apr 20, 2017</td><td>0</td><td>[ted, nugent, smi...</td></tr>\n",
       "<tr><td>BUSTED! ONE OF NA...</td><td>[i, wonder, if, t...</td><td>politics</td><td>Nov 2, 2016</td><td>0</td><td>[wonder, get, tre...</td></tr>\n",
       "<tr><td>Kenya parliament ...</td><td>[nairobi, reuters...</td><td>worldnews</td><td>October 11, 2017 </td><td>1</td><td>[nairobi, reuters...</td></tr>\n",
       "<tr><td>BRITISH TV PERSON...</td><td>[he, obama, makes...</td><td>politics</td><td>Dec 9, 2015</td><td>0</td><td>[obama, makes, wa...</td></tr>\n",
       "<tr><td>Mnangagwa vows to...</td><td>[harare, reuters,...</td><td>worldnews</td><td>November 23, 2017 </td><td>1</td><td>[harare, reuters,...</td></tr>\n",
       "<tr><td>Factbox - What&#x27;s ...</td><td>[madrid, reuters,...</td><td>worldnews</td><td>December 22, 2017 </td><td>1</td><td>[madrid, reuters,...</td></tr>\n",
       "<tr><td>Philippines&#x27; Dute...</td><td>[manila, reuters,...</td><td>worldnews</td><td>November 15, 2017 </td><td>1</td><td>[manila, reuters,...</td></tr>\n",
       "<tr><td>FEEL THE BERN: Su...</td><td>[it, would, appea...</td><td>politics</td><td>Jan 7, 2016</td><td>0</td><td>[would, appear, s...</td></tr>\n",
       "<tr><td> John McCain Goes...</td><td>[it, s, no, secre...</td><td>News</td><td>January 7, 2016</td><td>0</td><td>[secret, senator,...</td></tr>\n",
       "<tr><td>NRA gun rights gr...</td><td>[washington, reut...</td><td>politicsNews</td><td>October 7, 2016 </td><td>1</td><td>[washington, reut...</td></tr>\n",
       "<tr><td>Japanese woman, c...</td><td>[tokyo, reuters, ...</td><td>worldnews</td><td>December 27, 2017 </td><td>1</td><td>[tokyo, reuters, ...</td></tr>\n",
       "<tr><td> Democrat Hilario...</td><td>[on, wednesday, t...</td><td>News</td><td>June 22, 2016</td><td>0</td><td>[wednesday, democ...</td></tr>\n",
       "<tr><td> Racist Conservat...</td><td>[typically, child...</td><td>News</td><td>December 1, 2016</td><td>0</td><td>[typically, child...</td></tr>\n",
       "<tr><td>Senate approves t...</td><td>[washington, reut...</td><td>politicsNews</td><td>August 3, 2017 </td><td>1</td><td>[washington, reut...</td></tr>\n",
       "<tr><td>British princes m...</td><td>[london, reuters,...</td><td>worldnews</td><td>August 29, 2017 </td><td>1</td><td>[london, reuters,...</td></tr>\n",
       "<tr><td>MOTHER OF TERRORI...</td><td>[american, transl...</td><td>left-news</td><td>Nov 17, 2015</td><td>0</td><td>[american, transl...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+--------------------+------------+------------------+----+--------------------+\n",
       "|               title|         words_token|     subject|              date|flag|                text|\n",
       "+--------------------+--------------------+------------+------------------+----+--------------------+\n",
       "| Mueller Spokesma...|[trump, supporter...|        News| December 17, 2017|   0|[trump, supporter...|\n",
       "|JUST IN: FCC VOTE...|[today, the, fcc,...|    politics|      Dec 14, 2017|   0|[today, fcc, vote...|\n",
       "|(VIDEO) OBAMA ON ...|[the, stats, don,...|    politics|       May 5, 2015|   0|[stats, lie, one,...|\n",
       "|BREAKING VIDEO: T...|[the, attack, on,...|    politics|      Dec 19, 2016|   0|[attack, russian,...|\n",
       "|3 CONSERVATIVE CE...|[ted, nugent, is,...|   left-news|      Apr 20, 2017|   0|[ted, nugent, smi...|\n",
       "|BUSTED! ONE OF NA...|[i, wonder, if, t...|    politics|       Nov 2, 2016|   0|[wonder, get, tre...|\n",
       "|Kenya parliament ...|[nairobi, reuters...|   worldnews| October 11, 2017 |   1|[nairobi, reuters...|\n",
       "|BRITISH TV PERSON...|[he, obama, makes...|    politics|       Dec 9, 2015|   0|[obama, makes, wa...|\n",
       "|Mnangagwa vows to...|[harare, reuters,...|   worldnews|November 23, 2017 |   1|[harare, reuters,...|\n",
       "|Factbox - What's ...|[madrid, reuters,...|   worldnews|December 22, 2017 |   1|[madrid, reuters,...|\n",
       "|Philippines' Dute...|[manila, reuters,...|   worldnews|November 15, 2017 |   1|[manila, reuters,...|\n",
       "|FEEL THE BERN: Su...|[it, would, appea...|    politics|       Jan 7, 2016|   0|[would, appear, s...|\n",
       "| John McCain Goes...|[it, s, no, secre...|        News|   January 7, 2016|   0|[secret, senator,...|\n",
       "|NRA gun rights gr...|[washington, reut...|politicsNews|  October 7, 2016 |   1|[washington, reut...|\n",
       "|Japanese woman, c...|[tokyo, reuters, ...|   worldnews|December 27, 2017 |   1|[tokyo, reuters, ...|\n",
       "| Democrat Hilario...|[on, wednesday, t...|        News|     June 22, 2016|   0|[wednesday, democ...|\n",
       "| Racist Conservat...|[typically, child...|        News|  December 1, 2016|   0|[typically, child...|\n",
       "|Senate approves t...|[washington, reut...|politicsNews|   August 3, 2017 |   1|[washington, reut...|\n",
       "|British princes m...|[london, reuters,...|   worldnews|  August 29, 2017 |   1|[london, reuters,...|\n",
       "|MOTHER OF TERRORI...|[american, transl...|   left-news|      Nov 17, 2015|   0|[american, transl...|\n",
       "+--------------------+--------------------+------------+------------------+----+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f94d9533-351e-4b7a-a356-cbbf76aa76fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most freq words in data\n",
    "\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "df_freq = df.withColumn(\"text\", concat_ws(\" \", df[\"text\"]))\n",
    "\n",
    "# df_freq.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f3d26157-f87e-4b99-9199-5bfb2ecdf63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent fake words\n",
    "\n",
    "df_fake_freq = df_freq.filter(df[\"flag\"] == 0)\n",
    "\n",
    "df_fake_text = df_fake_freq.select(\"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7164bfbe-2673-4769-8b62-75f7c79cca61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 22:06:15 WARN TaskSetManager: Stage 73 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 22:06:17 WARN TaskSetManager: Stage 74 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     word|count|\n",
      "+---------+-----+\n",
      "|    trump|73933|\n",
      "|     said|31013|\n",
      "|   people|25963|\n",
      "|president|25586|\n",
      "|    would|23427|\n",
      "|      one|22935|\n",
      "|       us|22049|\n",
      "|  clinton|18011|\n",
      "|    obama|17813|\n",
      "|     like|17621|\n",
      "|   donald|17215|\n",
      "|     also|15242|\n",
      "|      new|14158|\n",
      "|     news|14126|\n",
      "|     even|13717|\n",
      "|  hillary|13565|\n",
      "|    white|12778|\n",
      "|     time|12728|\n",
      "|    state|12525|\n",
      "|      via|11273|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_fake_text = df_fake_text\\\n",
    ".withColumn(\"words\", split(col(\"text\"), \" \"))\\\n",
    ".where(length(\"text\")>0)\n",
    "\n",
    "df_freq_fake = df_fake_text\\\n",
    ".select(explode(col(\"words\")).alias(\"word\"))\\\n",
    ".groupBy(\"word\").count()\\\n",
    "\n",
    "df_freq_fake.orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e1f3aac-3764-4da3-8b94-d1c24743549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#most true words\n",
    "\n",
    "df_true_freq = df_freq.filter(df[\"flag\"] == 1)\n",
    "\n",
    "df_true_text = df_true_freq.select(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d810c99-54a3-4764-b88e-cf005d1c8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 19:45:21 WARN TaskSetManager: Stage 41 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 19:45:32 WARN TaskSetManager: Stage 42 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|      said|98985|\n",
      "|     trump|54341|\n",
      "|        us|41137|\n",
      "|     state|36638|\n",
      "|     would|31514|\n",
      "|    presid|28367|\n",
      "|    reuter|28306|\n",
      "|republican|22108|\n",
      "|    govern|20215|\n",
      "|      year|19294|\n",
      "|      hous|17525|\n",
      "|       new|16786|\n",
      "|      unit|16525|\n",
      "|  democrat|16214|\n",
      "|      also|15944|\n",
      "|       say|15942|\n",
      "|     senat|15705|\n",
      "|     elect|15524|\n",
      "|     peopl|15322|\n",
      "|     parti|15002|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_true_text = df_true_text\\\n",
    ".withColumn(\"words\", split(col(\"text\"), \" \"))\\\n",
    ".where(length(\"text\")>0)\n",
    "\n",
    "df_freq_true = df_true_text\\\n",
    ".select(explode(col(\"words\")).alias(\"word\"))\\\n",
    ".groupBy(\"word\").count()\\\n",
    "\n",
    "df_freq_true.orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eec22931-121d-4e9d-849d-f660964bfd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- words_token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- flag: integer (nullable = false)\n",
      " |-- text: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_freq.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7e0b522-c6ba-463c-9d98-24dfcf61768a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 19:45:53 WARN TaskSetManager: Stage 48 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44898"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_freq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a459d10-e4df-43a9-a0b9-20d2d87c0a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subject: string (nullable = true)\n",
      " |-- flag: integer (nullable = false)\n",
      " |-- text: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop unwanted coloumns\n",
    "\n",
    "df_ml = df_freq.drop(\"title\",\"words_token\",\"date\")\n",
    "df_ml.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66c936e2-3d6e-4f62-bbf2-ca93547a3ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 19:46:22 WARN TaskSetManager: Stage 51 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 19:46:25 WARN TaskSetManager: Stage 52 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 19:46:52 WARN TaskSetManager: Stage 55 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 19:46:56 WARN TaskSetManager: Stage 56 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# x values and Y values\n",
    "\n",
    "x = df_ml.select('text').rdd.flatMap(lambda x: x).collect()\n",
    "y = df_ml.select('flag').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b78c8d4-0c75-4a24-9658-be065015fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f5d52f9-5e40-40ed-a9e6-15e1aff35790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peopl petit start juli th ask govern formal design black live matter terrorist organ white hous requir person group initi petit peopl petit page must gather least signatur day signatur white hous site guarante review petit offer offici respons obama doubt spin meantim statement american citizen belief black live matter noth terror organ overwhelm support label black live matter terror group bode well hillari open support embarrass effort pander votesfrom white hous creat peopl petit pageabout peopleth right petit govern guarante first amend unit state constitut peopl platform empow american public take action like never way anybodi anywher speak direct govern becom agent changewith peopl easili creat petit onlin share collect signatur gather signatur day review petit make sure get front appropri polici expert issu offici responsepetit potenti enact real chang check petit creator success stori also fundament right american citizen opportun connect communiti likemind peopl invest make chang ideal run petit peopl start someth bigger longterm robust form civic engagementth white hous review respondsonc petit reach requir threshold signatur put queue review white hous other still sign petit await respons white hous respond everyon sign petit get email white hous let know review respond petitionq white hous decid petit respond toth white hous plan respond petit cross current signatur threshold view term particip page case white hous respons might address fact particular matter avoid appear improp influenc specif procur law enforc adjudicatori matter addit white hous respond petit violat peopl term particip case singl updat may use petit similar topicsto date petit collect signatur like add name click\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fa9c01d-6bc6-4aa2-88e2-bec5be2ffe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize the text\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88a581c7-1eff-412c-b461-6e3e18d24183",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization = TfidfVectorizer()\n",
    "xv_train = vectorization.fit_transform(x_train)\n",
    "xv_test = vectorization.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35d5fc64-0068-4a86-a0af-671fbbaa7a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<33673x157070 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5098041 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xv_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e77b3db7-eb5f-4f78-8238-074c846fdc5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xv_train.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d640734a-d7dc-48aa-8d6f-07112b6970ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15be177c-bd0b-441e-a4b6-864909c0414f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LogisticRegression()\n",
    "LR.fit(xv_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1aab4ff-7b57-4d6c-81da-53d8273c1ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.986369710467706"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.score(xv_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06daee6b-4065-4f34-b7bc-22a90f2b7452",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_LR = LR.predict(xv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ba3e649-b971-40ae-a73f-f019d4048097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      5877\n",
      "           1       0.98      0.99      0.99      5348\n",
      "\n",
      "    accuracy                           0.99     11225\n",
      "   macro avg       0.99      0.99      0.99     11225\n",
      "weighted avg       0.99      0.99      0.99     11225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d1ef1ea-c1af-41b5-b4db-3b60a3a7a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db253665-1ce8-4dcf-9189-297f7544ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def output_lable(n):\n",
    "    if n == 0:\n",
    "        return \"Fake News\"\n",
    "    elif n == 1:\n",
    "        return \"Not A Fake News\"\n",
    "    \n",
    "def manual_testing(news):\n",
    "    testing_news = {\"text\":[news]}\n",
    "    new_def_test = pd.DataFrame(testing_news)\n",
    "    new_def_test[\"text\"] = new_def_test[\"text\"].apply(wordopt) \n",
    "    new_x_test = new_def_test[\"text\"]\n",
    "    new_xv_test = vectorization.transform(new_x_test)\n",
    "    pred_LR = LR.predict(new_xv_test)\n",
    "    print(\"*****\")\n",
    "    #print(pred_LR[0])\n",
    "    print(new_xv_test)\n",
    "    print(type(pred_LR[0]))\n",
    "    # pred_DT = DT.predict(new_xv_test)\n",
    "    # pred_GBC = GBC.predict(new_xv_test)\n",
    "    # pred_RFC = RFC.predict(new_xv_test)\n",
    "\n",
    "    return print(\"\\n\\nLR Prediction: {} \\n\".format(output_lable(pred_LR[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b36ae219-cfc4-48b7-93e0-562e30fea614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " The following statements were posted to the verified Twitter accounts of U.S. President Donald Trump, @realDonaldTrump and @POTUS.  The opinions expressed are his own. Reuters has not edited the statements or confirmed their accuracy.  @realDonaldTrump : - Vanity Fair, which looks like it is on its last legs, is bending over backwards in apologizing for the minor hit they took at Crooked H. Anna Wintour, who was all set to be Amb to Court of St James’s & a big fundraiser for CH, is beside herself in grief & begging for forgiveness! [1024 EST] -- Source link: (bit.ly/2jBh4LU) (bit.ly/2jpEXYR)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "0\n",
      "  (0, 152967)\t0.16096178767446143\n",
      "  (0, 143780)\t0.054190873866555564\n",
      "  (0, 142249)\t0.02866970596549606\n",
      "  (0, 140603)\t0.04939542872986928\n",
      "  (0, 140058)\t0.5176326165631658\n",
      "  (0, 130228)\t0.06689479292151192\n",
      "  (0, 124619)\t0.052321727153028616\n",
      "  (0, 113471)\t0.14800674609052442\n",
      "  (0, 108409)\t0.1725442055210553\n",
      "  (0, 107617)\t0.09057757025271987\n",
      "  (0, 97681)\t0.08030345818911086\n",
      "  (0, 97350)\t0.11235736170548934\n",
      "  (0, 96237)\t0.1677370677824023\n",
      "  (0, 95306)\t0.16400836207085898\n",
      "  (0, 94316)\t0.32192357534892285\n",
      "  (0, 92587)\t0.1793194856289962\n",
      "  (0, 85044)\t0.06598028632480062\n",
      "  (0, 79234)\t0.32801672414171795\n",
      "  (0, 77282)\t0.0654216862280709\n",
      "  (0, 77004)\t0.033483274037078736\n",
      "  (0, 74638)\t0.035840970070839646\n",
      "  (0, 67735)\t0.1793194856289962\n",
      "  (0, 64352)\t0.3048518884485302\n",
      "  (0, 58713)\t0.06287071507041829\n",
      "  (0, 53124)\t0.11647021989379128\n",
      "  (0, 44061)\t0.06961458558868093\n",
      "  (0, 36815)\t0.0317254189439036\n",
      "  (0, 30271)\t0.1677370677824023\n",
      "  (0, 29327)\t0.05127258504891143\n",
      "  (0, 21784)\t0.14084352637767122\n",
      "  (0, 14773)\t0.15145314609904822\n",
      "  (0, 14247)\t0.056944544857923336\n",
      "  (0, 12296)\t0.10797328192517211\n",
      "  (0, 6257)\t0.12424639176539187\n",
      "  (0, 5177)\t0.15838594572462691\n",
      "  (0, 4104)\t0.16400836207085898\n",
      "<class 'numpy.int64'>\n",
      "\n",
      "\n",
      "LR Prediction: Fake News \n",
      "\n"
     ]
    }
   ],
   "source": [
    "news = str(input())\n",
    "manual_testing(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5614960a-c052-4dca-91ab-08778e0fde38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
