{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b650e29-a034-47ac-84d3-6816292e60fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 22:42:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/04 22:42:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/12/04 22:42:29 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poojaakki/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "# conf.set('spark.ui.proxyBase'\n",
    "# , '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4041')\n",
    "conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "conf.set('spark.driver.memory','4g')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.SQLContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b1c6588-561f-474f-a31d-317c9e965c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "schema= StructType(\n",
    "      [StructField('title',StringType(),True),\n",
    "      StructField('text',StringType(),True),\n",
    "      StructField('subject',StringType(),True),\n",
    "      StructField('date',StringType(),True)])\n",
    "\n",
    "df0 = pd.read_csv('/Users/poojaakki/Desktop/projects/Fake-News-Detection-Data-Analysis/datasets/Kaggle/fake.csv')\n",
    "df1 = pd.read_csv('/Users/poojaakki/Desktop/projects/Fake-News-Detection-Data-Analysis/datasets/Kaggle/true.csv')\n",
    "df_fake = spark.createDataFrame(df0,schema=schema)\n",
    "df_true = spark.createDataFrame(df1,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a29b36a1-7979-46a3-b97b-d941fec7ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a label to the data for fake news as 0 and true news as 1 and shuffle using rand\n",
    "from pyspark.sql.functions import lit, rand\n",
    "\n",
    "df_concat = df_true.withColumn('flag', lit(1)).union(df_fake.withColumn('flag', lit(0))).orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f50cff9b-6905-4347-828c-2e447cc4d2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 22:55:47 WARN TaskSetManager: Stage 13 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/04 22:55:48 WARN TaskSetManager: Stage 14 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>text</th><th>subject</th><th>flag</th></tr>\n",
       "<tr><td>Tucker Carlson de...</td><td>left-news</td><td>0</td></tr>\n",
       "<tr><td>BRUSSELS (Reuters...</td><td>worldnews</td><td>1</td></tr>\n",
       "<tr><td>LONDON (Reuters) ...</td><td>worldnews</td><td>1</td></tr>\n",
       "<tr><td>Barack Hussein Ob...</td><td>left-news</td><td>0</td></tr>\n",
       "<tr><td>NEW YORK (Reuters...</td><td>worldnews</td><td>1</td></tr>\n",
       "<tr><td>LONDON (Reuters) ...</td><td>worldnews</td><td>1</td></tr>\n",
       "<tr><td>Remember that cut...</td><td>News</td><td>0</td></tr>\n",
       "<tr><td>CAIRO (Reuters) -...</td><td>politicsNews</td><td>1</td></tr>\n",
       "<tr><td>ROME (Reuters) - ...</td><td>worldnews</td><td>1</td></tr>\n",
       "<tr><td>AUSTIN, Texas/WAS...</td><td>politicsNews</td><td>1</td></tr>\n",
       "<tr><td>CODY, Wyo. (Reute...</td><td>politicsNews</td><td>1</td></tr>\n",
       "<tr><td>ISLAMABAD (Reuter...</td><td>worldnews</td><td>1</td></tr>\n",
       "<tr><td>(Reuters) - Outsi...</td><td>politicsNews</td><td>1</td></tr>\n",
       "<tr><td>SAN FRANCISCO (Re...</td><td>politicsNews</td><td>1</td></tr>\n",
       "<tr><td>WASHINGTON (Reute...</td><td>politicsNews</td><td>1</td></tr>\n",
       "<tr><td>WASHINGTON (Reute...</td><td>politicsNews</td><td>1</td></tr>\n",
       "<tr><td>THE HUNTINGTON BE...</td><td>politics</td><td>0</td></tr>\n",
       "<tr><td>It s been a bumpy...</td><td>politics</td><td>0</td></tr>\n",
       "<tr><td>Obama s ICE Direc...</td><td>left-news</td><td>0</td></tr>\n",
       "<tr><td>A Muslim activist...</td><td>politics</td><td>0</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+------------+----+\n",
       "|                text|     subject|flag|\n",
       "+--------------------+------------+----+\n",
       "|Tucker Carlson de...|   left-news|   0|\n",
       "|BRUSSELS (Reuters...|   worldnews|   1|\n",
       "|LONDON (Reuters) ...|   worldnews|   1|\n",
       "|Barack Hussein Ob...|   left-news|   0|\n",
       "|NEW YORK (Reuters...|   worldnews|   1|\n",
       "|LONDON (Reuters) ...|   worldnews|   1|\n",
       "|Remember that cut...|        News|   0|\n",
       "|CAIRO (Reuters) -...|politicsNews|   1|\n",
       "|ROME (Reuters) - ...|   worldnews|   1|\n",
       "|AUSTIN, Texas/WAS...|politicsNews|   1|\n",
       "|CODY, Wyo. (Reute...|politicsNews|   1|\n",
       "|ISLAMABAD (Reuter...|   worldnews|   1|\n",
       "|(Reuters) - Outsi...|politicsNews|   1|\n",
       "|SAN FRANCISCO (Re...|politicsNews|   1|\n",
       "|WASHINGTON (Reute...|politicsNews|   1|\n",
       "|WASHINGTON (Reute...|politicsNews|   1|\n",
       "|THE HUNTINGTON BE...|    politics|   0|\n",
       "|It s been a bumpy...|    politics|   0|\n",
       "|Obama s ICE Direc...|   left-news|   0|\n",
       "|A Muslim activist...|    politics|   0|\n",
       "+--------------------+------------+----+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat =  df_concat.drop(\"date\", \"title\")\n",
    "\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3623f259-6cbc-4de0-a2e4-36dc8d92c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to avaoid the confusion convert everything to lowercase\n",
    "from pyspark.sql.functions import col, split, lower, regexp_replace, length\n",
    "\n",
    "df_concat = df_concat\\\n",
    ".withColumn(\"text\", regexp_replace(lower(col(\"text\")), r\"[^0-9a-z]\", \" \"))\\\n",
    ".where(length(\"text\")>0)\n",
    "\n",
    "#df_concat.show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2795aaf-6aaf-4362-a7cc-2555d34452d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/poojaakki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# remove stop words from it\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22f7d39f-f224-43c0-940d-bd1a164ef8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_concat = df_concat.withColumn('words',F.split(F.col('text'),' '))\n",
    "# df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "358ed650-5014-48ca-ab8d-4be12edf84d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover \n",
    "\n",
    "stopwordsRemovalFeature = StopWordsRemover(inputCol=\"words\", \n",
    "                   outputCol=\"words without stop\").setStopWords(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c2166ce-a85d-4bfc-b615-e7f229c436d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stopWordRemovalPipeline = Pipeline(stages=[stopwordsRemovalFeature])\n",
    "pipelineFitRemoveStopWords = stopWordRemovalPipeline.fit(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bb7d044-466d-42b9-91da-56914288d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pipelineFitRemoveStopWords.transform(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fde5b3d8-a36b-45d8-90b0-1b11c7de2787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert array of strings to strings\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "df_concat = df_concat.withColumn(\"text1\", concat_ws(\" \", df_concat[\"words without stop\"]))\n",
    "# df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b55509b-2f42-403e-a175-079c49017940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = df_concat.drop(\"words\",\"words without stop\")\n",
    "\n",
    "# df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "735286c3-7ffd-4489-b23e-37a137e0e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = df_concat.drop(\"text\")\n",
    "\n",
    "# df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f81b631-74d8-4ed8-b0ac-d630a4db79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove space\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "def space_removal(f):\n",
    "    clean_tokens = []\n",
    "    for tok in f:\n",
    "        if tok:\n",
    "            clean_tokens.append(tok)\n",
    "    return clean_tokens\n",
    "\n",
    "udf_space_removal = udf(space_removal, ArrayType(StringType()))\n",
    "\n",
    "df_concat = df_concat.withColumn('text1', udf_space_removal(f.col('text1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "407f2dbc-acbf-4eb7-8904-459945aae598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 22:56:35 WARN TaskSetManager: Stage 19 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 22:56:36 WARN TaskSetManager: Stage 20 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+--------------------+\n",
      "|     subject|flag|               text1|\n",
      "+------------+----+--------------------+\n",
      "|   left-news|   0|[t, u, c, k, e, r...|\n",
      "|   worldnews|   1|[b, r, u, s, s, e...|\n",
      "|   worldnews|   1|[l, o, n, d, o, n...|\n",
      "|   left-news|   0|[b, a, r, a, c, k...|\n",
      "|   worldnews|   1|[n, e, w,  , y, o...|\n",
      "|   worldnews|   1|[l, o, n, d, o, n...|\n",
      "|        News|   0|[r, e, m, e, m, b...|\n",
      "|politicsNews|   1|[c, a, i, r, o,  ...|\n",
      "|   worldnews|   1|[r, o, m, e,  ,  ...|\n",
      "|politicsNews|   1|[a, u, s, t, i, n...|\n",
      "|politicsNews|   1|[c, o, d, y,  ,  ...|\n",
      "|   worldnews|   1|[i, s, l, a, m, a...|\n",
      "|politicsNews|   1|[ , r, e, u, t, e...|\n",
      "|politicsNews|   1|[s, a, n,  , f, r...|\n",
      "|politicsNews|   1|[w, a, s, h, i, n...|\n",
      "|politicsNews|   1|[w, a, s, h, i, n...|\n",
      "|    politics|   0|[h, u, n, t, i, n...|\n",
      "|    politics|   0|[b, u, m, p, y,  ...|\n",
      "|   left-news|   0|[o, b, a, m, a,  ...|\n",
      "|    politics|   0|[m, u, s, l, i, m...|\n",
      "+------------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_concat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "102fbda7-735c-452e-b0eb-10b072330ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = df_concat.withColumnRenamed(\"text1\",\"text\")\n",
    "\n",
    "# df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35b22c26-b084-45ad-83e9-d379bd6da6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent words in fake news\n",
    "\n",
    "df_fake = df_concat.filter(df_concat[\"flag\"] == 0)\n",
    "\n",
    "df_fake_text = df_fake.select(\"text\")\n",
    "\n",
    "# df_fake_text.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33da0788-69ac-45d7-b01c-20ab23431eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 22:50:10 WARN TaskSetManager: Stage 6 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 22:50:18 WARN TaskSetManager: Stage 7 contains a task of very large size (4983 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|     word|  count|\n",
      "+---------+-------+\n",
      "|         |1358755|\n",
      "|    trump|  79352|\n",
      "|     said|  33763|\n",
      "|president|  27715|\n",
      "|   people|  26570|\n",
      "|      one|  24531|\n",
      "|    would|  23562|\n",
      "|  clinton|  19173|\n",
      "|    obama|  18797|\n",
      "|     like|  18098|\n",
      "|   donald|  17681|\n",
      "|     also|  15403|\n",
      "|       us|  14822|\n",
      "|     news|  14629|\n",
      "|      new|  14394|\n",
      "|  hillary|  14122|\n",
      "|     even|  14011|\n",
      "|     time|  13870|\n",
      "|    state|  13465|\n",
      "|    white|  13189|\n",
      "+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_fake_text = df_fake_text\\\n",
    ".withColumn(\"words\", split(col(\"text\"), \" \"))\\\n",
    ".where(length(\"text\")>0)\n",
    "\n",
    "df_freq_fake = df_fake_text\\\n",
    ".select(explode(col(\"words\")).alias(\"word\"))\\\n",
    ".groupBy(\"word\").count()\\\n",
    "\n",
    "df_freq_fake.orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8388d16-f485-4954-93b5-a0beb3a1e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent words in true news\n",
    "\n",
    "df_true = df_concat.filter(df_concat[\"Flag\"] == 1)\n",
    "\n",
    "df_true_text = df_true.select(\"text\")\n",
    "\n",
    "# df_true_text.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b9f36-d3c5-4a06-96a8-5863a4a2219f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
